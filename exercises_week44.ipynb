{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55f7cd56",
   "metadata": {},
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html exercisesweek44.do.txt  -->\n",
    "<!-- dom:TITLE: Exercises week 44 -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c83276",
   "metadata": {},
   "source": [
    "# Exercises week 44\n",
    "\n",
    "**October 27-31, 2025**\n",
    "\n",
    "Date: **Deadline is Friday October 31 at midnight**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a26983",
   "metadata": {},
   "source": [
    "# Overarching aims of the exercises this week\n",
    "\n",
    "The exercise set this week has two parts.\n",
    "\n",
    "1. The first is a version of the exercises from week 39, where you got started with the report and github repository for project 1, only this time for project 2. This part is required, and a short feedback to this exercise will be available before the project deadline. And you can reuse these elements in your final report.\n",
    "\n",
    "2. The second is a list of questions meant as a summary of many of the central elements we have discussed in connection with projects 1 and 2, with a slight bias towards deep learning methods and their training. The hope is that these exercises can be of use in your discussions about the neural network results in project 2. **You don't need to answer all the questions, but you should be able to answer them by the end of working on project 2.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350c58e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Deliverables\n",
    "\n",
    "First, join a group in canvas with your group partners. Pick an avaliable group for Project 2 in the “People” page. If you don't have a group, you should really consider joining one!\n",
    "\n",
    "Complete exercise 1 while working in an Overleaf project. Then, in canvas, include\n",
    "\n",
    "- An exported PDF of the report draft you have been working on.\n",
    "- A comment linking to the github repository used in exercise **1d)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f65f6e",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "\n",
    "Following the same directions as in the weekly exercises for week 39:\n",
    "\n",
    "**a)** Create a report document in Overleaf, and write a suitable abstract and introduction for project 2.\n",
    "\n",
    "**b)** Add a figure in your report of a heatmap showing the test accuracy of a neural network with [0, 1, 2, 3] hidden layers and [5, 10, 25, 50] nodes per hidden layer.\n",
    "\n",
    "**c)** Add a figure in your report which meets as few requirements as possible of what we consider a good figure in this course, while still including some results, a title, figure text, and axis labels. Describe in the text of the report the different ways in which the figure is lacking. (This should not be included in the final report for project 2.)\n",
    "\n",
    "**d)** Create a github repository or folder in a repository with all the elements described in exercise 4 of the weekly exercises of week 39.\n",
    "\n",
    "**e)** If applicable, add references to your report for the source of your data for regression and classification, the source of claims you make about your data, and for the sources of the gradient optimizers you use and your general claims about these.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dff53b8",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "\n",
    "**a)** Linear and logistic regression methods\n",
    "\n",
    "1. What is the main difference between ordinary least squares and Ridge regression?\n",
    "\n",
    "2. Which kind of data set would you use logistic regression for?\n",
    "\n",
    "3. In linear regression you assume that your output is described by a continuous non-stochastic function $f(x)$. Which is the equivalent function in logistic regression?\n",
    "\n",
    "4. Can you find an analytic solution to a logistic regression type of problem?\n",
    "\n",
    "5. What kind of cost function would you use in logistic regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae4c3d5",
   "metadata": {},
   "source": [
    "<u>**Answer:**</u>\n",
    "\n",
    "1. The main difference is that Ridge have a so called regularization term that are added to the prediction error in the cost function (MSE). The regularization term, $\\lambda_{\\text{Ridge}}$, scales the sum of the squared model parameters, before its added to the cost of the prediction. In this manner large parameter values will increase the cost function and thereby leading to smaller parameters values.\n",
    "\n",
    "2. For logistic regression we would use a dataset consisting of binary targets, meaning given a number of independent variables $\\boldsymbol{x}$, the dependant target variable $y$ take on of two values, e.g. yes/no, effective/non-effective, represented by 1 and 0.\n",
    "\n",
    "3. The equivalent function in logistic regression is the logistic function, which is configured to take real values between 0 and 1, which can be interpreted as giving a probability of the target value being 1.\n",
    "\n",
    "4. In general, logistic regression problems do not have analytic solutions and numerical methods must be used.\n",
    "\n",
    "5. For logistic regression we use the cross entropy, i.e. the negative log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a056a4",
   "metadata": {},
   "source": [
    "**b)** Deep learning\n",
    "\n",
    "1. What is an activation function and discuss the use of an activation function? Explain three different types of activation functions?\n",
    "\n",
    "2. Describe the architecture of a typical feed forward Neural Network (NN).\n",
    "\n",
    "3. You are using a deep neural network for a prediction task. After training your model, you notice that it is strongly overfitting the training set and that the performance on the test isn’t good. What can you do to reduce overfitting?\n",
    "\n",
    "4. How would you know if your model is suffering from the problem of exploding gradients?\n",
    "\n",
    "5. Can you name and explain a few hyperparameters used for training a neural network?\n",
    "\n",
    "6. Describe the architecture of a typical Convolutional Neural Network (CNN)\n",
    "\n",
    "7. What is the vanishing gradient problem in Neural Networks and how to fix it?\n",
    "\n",
    "8. When it comes to training an artificial neural network, what could the reason be for why the cost/loss doesn't decrease in a few epochs?\n",
    "\n",
    "9. How does L1/L2 regularization affect a neural network?\n",
    "\n",
    "10. What is(are) the advantage(s) of deep learning over traditional methods like linear regression or logistic regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aa9ac8",
   "metadata": {},
   "source": [
    "<u>**Answer:**</u>\n",
    "\n",
    "1. The activation function is the function applied to each node's weighted sum of the inputs from the previous layer plus the node's bias. The activation function is what makes the neural networks work; without activation functions the network's output is merely a linear combination of the input data. ReLU is a common acitvation function that returns 0 if the input is negative and is the identity otherwise. Another common function is the sigmoid function (also called the logistic function). The sigmoid function takes values between 0 and 1 and is shaped vaguely like an S, hence the name. Softsign is takes values between -1 and 1 and is also sigmoid-shaped.\n",
    "\n",
    "2. You have a given number of input values and a given number of output variables. In between there are output layers of various sizes. Each layer consists of a specified number of nodes. At each node we compute the weighted sum of the of input values and add a node-specific bias. To this sum we apply an activation function that is specified separately for each layer. The output of the activation functions for each node is then used as the input for the next layer. This is repeated until we reach the output layer where the network's output can be read off.\n",
    "\n",
    "3. One can reduce the model complexity, e.g. the number of layers.\n",
    "\n",
    "4. Your outputs are nonsensical and the model parameters are exploding.\n",
    "\n",
    "5. Learning rate, this indicates how quickly we change the model parameters, i.e. how much impact the gradient has on the change in the model parameters. Batch size, the number of randomly selected data points used to compute an estimate of the gradient in training, smaller numbers introduces noise but increases training speed. Drop out, gives a probability of each neuron being left out when updating the gradient during each training step.\n",
    "\n",
    "6. __\n",
    "\n",
    "7. Vanishing gradient problem is when the gradient is calculated by backpropagation and through repeated multiplication of possibly small numbers and thereby becomes so small for the first layers such that training effectively ceases for them. This can be somewhat amended by using adaptive gradient descent methods, increasing the learning rate, or most importantly picking activation functions that are better behaved. In addition, batch normalization, which is a method that standardizes the inputs between each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c48bc09",
   "metadata": {},
   "source": [
    "**c)** Optimization part\n",
    "\n",
    "1. Which is the basic mathematical root-finding method behind essentially all gradient descent approaches(stochastic and non-stochastic)?\n",
    "\n",
    "2. And why don't we use it? Or stated differently, why do we introduce the learning rate as a parameter?\n",
    "\n",
    "3. What might happen if you set the momentum hyperparameter too close to 1 (e.g., 0.9999) when using an optimizer for the learning rate?\n",
    "\n",
    "4. Why should we use stochastic gradient descent instead of plain gradient descent?\n",
    "\n",
    "5. Which parameters would you need to tune when use a stochastic gradient descent approach?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76de155",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "1. Newton-Raphson method.\n",
    "2. So that we can tune the size of the steps to prevent too large or too small steps that can lead to divergent solutions or too slow convergence.\n",
    "3. The steps become too big and can end up going in the wrong direction.\n",
    "4. It is quicker for large data sets and the introduced noise can help avoid getting stuck at local minima.\n",
    "5. The batch size and the learning rate, as well as whatever other parameters your gradient descent method may have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b0b5f6",
   "metadata": {},
   "source": [
    "**d)** Analysis of results\n",
    "\n",
    "1. How do you assess overfitting and underfitting?\n",
    "\n",
    "2. Why do we divide the data in test and train and/or eventually validation sets?\n",
    "\n",
    "3. Why would you use resampling methods in the data analysis? Mention some widely popular resampling methods.\n",
    "\n",
    "4. Why might a model that does not overfit the data (maybe because there is a lot of data) perform worse when we add regularization?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afafaeba",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "1. At the most basic you compare the performance of the model on the test and training set. Underfitting would see bad performance in both cases. Overfitting would see good performance in the training case and significantly worse performance in the test case. You can also run a bias-variance trade off analysis.\n",
    "2. In order to asses the generalization of the data and if it is overfitted.\n",
    "3. \n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b08528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
